{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports necessary modules and packages from TensorFlow and NumPy. It also disables warnings to ensure a cleaner output.\n",
    "\n",
    "The CIFAR-10 dataset is a collection of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It's commonly used for image classification tasks. TensorFlow provides a convenient way to load this dataset using the cifar10.load_data() function.\n",
    "\n",
    "The CIFAR-10 dataset is divided into two sets: training and testing. The training set is used to train the model, while the testing set is used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration parameters for training the ResNet model are defined. The model_type is set to 'ResNet', indicating the type of model to be trained. The batch_size is specified as 32, representing the number of samples per gradient update during training. Training will be conducted over 200 epochs, with the option for data_augmentation set to True, which enables the augmentation of training data to improve model generalization. The num_classes parameter is set to 10, representing the number of classes in the dataset.\n",
    "\n",
    "To enhance model performance, subtract_pixel_mean is enabled, which subtracts the pixel mean from the input data. This preprocessing step can help normalize the data and improve convergence during training. Additionally, the model's depth is determined by the n parameter, which controls the number of residual blocks in each stage. The version parameter specifies the version of the ResNet model, and the depth is computed based on the supplied n, determining the overall number of layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'ResNet'\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 120\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "n = 3\n",
    "\n",
    "# Model version\n",
    "version = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "depth = n * 6 + 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 dataset is loaded and preprocessed for training the ResNet model. The dataset is loaded using the cifar10.load_data() function, and the training and testing sets are unpacked into (x_train, y_train) and (x_test, y_test) variables, respectively.\n",
    "\n",
    "The input_shape is determined based on the shape of the input images in the training data. Next, the data is normalized by dividing the pixel values by 255, converting them to floating-point numbers between 0 and 1.\n",
    "\n",
    "If subtract_pixel_mean is enabled, the mean pixel value is computed from the training data and subtracted from both the training and testing sets. This step ensures that the pixel mean is subtracted from each image, aiding in normalization and improving convergence during training.\n",
    "\n",
    "Finally, the class labels are converted to binary class matrices using one-hot encoding, where each class label is represented as a binary vector with a 1 at the index corresponding to the class and 0s elsewhere. This transformation is performed using the tf.keras.utils.to_categorical() function, enabling categorical classification during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lr_schedule` function defines a learning rate schedule for adjusting the learning rate during training epochs. This schedule is designed to reduce the learning rate at specific epochs to improve model convergence and performance. The function takes the current epoch number as input and returns the corresponding learning rate.\n",
    "\n",
    "At the beginning of training, the initial learning rate `lr` is set to `1e-3` (0.001). Then, the function checks the current epoch number to determine whether to adjust the learning rate. \n",
    "\n",
    "- If the epoch is greater than 180, the learning rate is reduced by a factor of `0.5e-3`.\n",
    "- If the epoch is greater than 160, the learning rate is reduced by a factor of `1e-3`.\n",
    "- If the epoch is greater than 120, the learning rate is reduced by a factor of `1e-2`.\n",
    "- If the epoch is greater than 80, the learning rate is reduced by a factor of `1e-1`.\n",
    "\n",
    "After calculating the new learning rate based on the epoch, the function prints the updated learning rate for monitoring purposes. Finally, the computed learning rate is returned to be used in the training process.\n",
    "\n",
    "This learning rate schedule allows for gradual reductions in the learning rate as training progresses, which can help stabilize training, prevent overfitting, and improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `resnet_layer` function constructs a 2D Convolution-Batch Normalization-Activation stack for building the ResNet model. This function is essential for creating the residual blocks within the network architecture. It provides flexibility in configuring convolutional layers with or without batch normalization and activation functions.\n",
    "\n",
    "- **Inputs**: \n",
    "  - `inputs`: Input tensor from the image or the previous layer.\n",
    "  - `num_filters`: Number of filters for the Conv2D layer.\n",
    "  - `kernel_size`: Size of the convolutional kernel.\n",
    "  - `strides`: Strides for the convolution operation.\n",
    "  - `activation`: Activation function to be applied.\n",
    "  - `batch_normalization`: Boolean indicating whether to include batch normalization.\n",
    "  - `conv_first`: Boolean indicating the order of operations.\n",
    "\n",
    "- **Returns**: \n",
    "  - `x`: Tensor as input to the next layer.\n",
    "\n",
    "This function starts by defining a Conv2D layer with specified parameters such as the number of filters, kernel size, strides, padding, and kernel initializer. Then, it applies batch normalization and activation functions according to the specified order (`conv_first`). If `conv_first` is True, convolution is applied first followed by batch normalization and activation. Otherwise, batch normalization and activation are applied before convolution.\n",
    "\n",
    "##### ResNet Version 1 Model\n",
    "\n",
    "The `resnet_v1` function constructs the ResNet Version 1 model, which consists of multiple residual blocks with stacking convolutional layers. This model architecture is known for its deep structure and skip connections, which facilitate training of very deep networks while mitigating issues like the vanishing gradient problem.\n",
    "\n",
    "- **Inputs**: \n",
    "  - `input_shape`: Shape of the input image tensor.\n",
    "  - `depth`: Number of core convolutional layers.\n",
    "  - `num_classes`: Number of classes (CIFAR10 has 10).\n",
    "\n",
    "- **Returns**: \n",
    "  - `model`: Keras model instance.\n",
    "\n",
    "This function first validates the depth parameter to ensure it follows the ResNet architecture guidelines. Then, it proceeds to define the model by stacking residual units with convolutional layers. The number of residual blocks and filters are determined based on the provided depth. Finally, the model concludes with an average pooling layer, a flatten layer, and a dense output layer with softmax activation for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = tf.keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After constructing the ResNet Version 1 model using the `resnet_v1` function with the specified input shape and depth, the code compiles the model for training. The compilation involves configuring the model with a loss function, an optimizer, and evaluation metrics.\n",
    "\n",
    "- **Loss Function**: Categorical cross-entropy is chosen as the loss function, suitable for multi-class classification tasks like CIFAR10.\n",
    "\n",
    "- **Optimizer**: Adam optimizer is utilized for optimizing the model parameters during training. The learning rate is initialized using the `lr_schedule` function with an initial epoch value of 0.\n",
    "\n",
    "- **Metrics**: Accuracy is selected as the evaluation metric to monitor the model's performance during training. It measures the proportion of correctly classified images among the total number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks and Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code prepares callbacks for model saving and learning rate adjustment during training. \n",
    "\n",
    "- **Model Saving Directory**: It specifies the directory where the trained model checkpoints will be saved. If the directory doesn't exist, it creates one.\n",
    "\n",
    "- **Model Checkpoint**: This callback monitors the validation accuracy during training and saves the model with the highest validation accuracy. It ensures that we have the best-performing model saved.\n",
    "\n",
    "- **Learning Rate Scheduler**: The learning rate scheduler adjusts the learning rate during training according to a predefined schedule. It is based on the `lr_schedule` function defined earlier.\n",
    "\n",
    "- **Learning Rate Reducer**: This callback reduces the learning rate when the validation loss has stopped improving, helping to fine-tune the training process.\n",
    "\n",
    "- **Callbacks List**: Finally, all the defined callbacks are added to a list, which will be passed to the `fit` method during model training. These callbacks will be executed at various stages of training to control the training process and save the model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name + \".keras\")\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responsible for training the model either with or without data augmentation. \n",
    "\n",
    "- If `data_augmentation` is set to `False`, the model trains on the original dataset. It calls the `fit` method on the model with the training data (`x_train`, `y_train`), validation data (`x_test`, `y_test`), batch size, number of epochs, and the defined callbacks.\n",
    "\n",
    "- If `data_augmentation` is set to `True`, real-time data augmentation is applied to the input data using the `ImageDataGenerator` provided by TensorFlow. Various augmentation techniques such as random shifts, flips, and rotations are applied to the input images to increase the diversity of the training data and improve the model's generalization ability. The `fit` method is then called on the model using the batches generated by `datagen.flow()`, along with the validation data and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YzxeLtFLmrN",
    "outputId": "82ab7ac1-12ab-41e7-85d6-bf3a8b0c872f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Learning rate:  0.001\n",
      "Epoch 1/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 1.5917 - accuracy: 0.4802\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50290, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.001.h5.keras\n",
      "1563/1563 [==============================] - 53s 25ms/step - loss: 1.5912 - accuracy: 0.4804 - val_loss: 1.6252 - val_accuracy: 0.5029 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 1.1820 - accuracy: 0.6361\n",
      "Epoch 2: val_accuracy improved from 0.50290 to 0.64830, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.002.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 1.1820 - accuracy: 0.6361 - val_loss: 1.1673 - val_accuracy: 0.6483 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 1.0143 - accuracy: 0.7002\n",
      "Epoch 3: val_accuracy improved from 0.64830 to 0.69540, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.003.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 1.0141 - accuracy: 0.7002 - val_loss: 1.0520 - val_accuracy: 0.6954 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.9196 - accuracy: 0.7374\n",
      "Epoch 4: val_accuracy improved from 0.69540 to 0.71390, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.004.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.9196 - accuracy: 0.7374 - val_loss: 0.9923 - val_accuracy: 0.7139 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.8567 - accuracy: 0.7604\n",
      "Epoch 5: val_accuracy did not improve from 0.71390\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.8567 - accuracy: 0.7604 - val_loss: 1.2797 - val_accuracy: 0.6595 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.8147 - accuracy: 0.7763\n",
      "Epoch 6: val_accuracy did not improve from 0.71390\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.8149 - accuracy: 0.7762 - val_loss: 1.1163 - val_accuracy: 0.6839 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.7790 - accuracy: 0.7906\n",
      "Epoch 7: val_accuracy improved from 0.71390 to 0.77330, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.007.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.7796 - accuracy: 0.7904 - val_loss: 0.8385 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 8/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.7520 - accuracy: 0.8002\n",
      "Epoch 8: val_accuracy did not improve from 0.77330\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.7521 - accuracy: 0.8001 - val_loss: 0.9266 - val_accuracy: 0.7485 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.8115\n",
      "Epoch 9: val_accuracy did not improve from 0.77330\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.7233 - accuracy: 0.8115 - val_loss: 0.8465 - val_accuracy: 0.7722 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 10/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.8201\n",
      "Epoch 10: val_accuracy did not improve from 0.77330\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.7008 - accuracy: 0.8201 - val_loss: 1.0316 - val_accuracy: 0.7182 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 11/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.8257\n",
      "Epoch 11: val_accuracy improved from 0.77330 to 0.82040, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.011.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6883 - accuracy: 0.8256 - val_loss: 0.7140 - val_accuracy: 0.8204 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 12/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6721 - accuracy: 0.8295\n",
      "Epoch 12: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6721 - accuracy: 0.8294 - val_loss: 0.8089 - val_accuracy: 0.7920 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 13/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.8376\n",
      "Epoch 13: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6598 - accuracy: 0.8377 - val_loss: 0.9255 - val_accuracy: 0.7643 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 14/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.8395\n",
      "Epoch 14: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6501 - accuracy: 0.8396 - val_loss: 0.7732 - val_accuracy: 0.8064 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 15/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.6374 - accuracy: 0.8457\n",
      "Epoch 15: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6374 - accuracy: 0.8457 - val_loss: 0.8309 - val_accuracy: 0.7966 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 16/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.8467\n",
      "Epoch 16: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6307 - accuracy: 0.8467 - val_loss: 0.7738 - val_accuracy: 0.8112 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 17/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.8516\n",
      "Epoch 17: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6227 - accuracy: 0.8515 - val_loss: 0.7326 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 18/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.8553\n",
      "Epoch 18: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6134 - accuracy: 0.8553 - val_loss: 0.7679 - val_accuracy: 0.8064 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 19/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.6088 - accuracy: 0.8570\n",
      "Epoch 19: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6088 - accuracy: 0.8570 - val_loss: 0.7969 - val_accuracy: 0.8039 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 20/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.6006 - accuracy: 0.8599\n",
      "Epoch 20: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.6007 - accuracy: 0.8599 - val_loss: 0.9523 - val_accuracy: 0.7743 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 21/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5947 - accuracy: 0.8607\n",
      "Epoch 21: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5947 - accuracy: 0.8607 - val_loss: 0.7575 - val_accuracy: 0.8175 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 22/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.8659\n",
      "Epoch 22: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5854 - accuracy: 0.8659 - val_loss: 0.9158 - val_accuracy: 0.7776 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 23/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5843 - accuracy: 0.8664\n",
      "Epoch 23: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5844 - accuracy: 0.8663 - val_loss: 0.8143 - val_accuracy: 0.8003 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 24/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.8675\n",
      "Epoch 24: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5811 - accuracy: 0.8674 - val_loss: 0.7638 - val_accuracy: 0.8181 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 25/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.8698\n",
      "Epoch 25: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5780 - accuracy: 0.8698 - val_loss: 0.8033 - val_accuracy: 0.8008 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 26/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.8717\n",
      "Epoch 26: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5689 - accuracy: 0.8717 - val_loss: 0.8795 - val_accuracy: 0.7827 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 27/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.8726\n",
      "Epoch 27: val_accuracy did not improve from 0.82040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5678 - accuracy: 0.8725 - val_loss: 1.1569 - val_accuracy: 0.7467 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 28/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5622 - accuracy: 0.8751\n",
      "Epoch 28: val_accuracy improved from 0.82040 to 0.84510, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.028.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5623 - accuracy: 0.8751 - val_loss: 0.6723 - val_accuracy: 0.8451 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 29/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.8753\n",
      "Epoch 29: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5626 - accuracy: 0.8753 - val_loss: 0.9384 - val_accuracy: 0.7754 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 30/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5571 - accuracy: 0.8773\n",
      "Epoch 30: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5572 - accuracy: 0.8773 - val_loss: 0.6945 - val_accuracy: 0.8360 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 31/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5596 - accuracy: 0.8748\n",
      "Epoch 31: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5599 - accuracy: 0.8748 - val_loss: 0.9840 - val_accuracy: 0.7699 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 32/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5524 - accuracy: 0.8789\n",
      "Epoch 32: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5523 - accuracy: 0.8789 - val_loss: 0.7746 - val_accuracy: 0.8149 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 33/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5523 - accuracy: 0.8777\n",
      "Epoch 33: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5522 - accuracy: 0.8778 - val_loss: 0.7100 - val_accuracy: 0.8387 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 34/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.8810\n",
      "Epoch 34: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5441 - accuracy: 0.8810 - val_loss: 0.6742 - val_accuracy: 0.8401 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 35/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5445 - accuracy: 0.8837\n",
      "Epoch 35: val_accuracy did not improve from 0.84510\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5444 - accuracy: 0.8837 - val_loss: 0.8522 - val_accuracy: 0.7984 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 36/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.8817\n",
      "Epoch 36: val_accuracy improved from 0.84510 to 0.85070, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.036.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5412 - accuracy: 0.8818 - val_loss: 0.6475 - val_accuracy: 0.8507 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 37/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5399 - accuracy: 0.8830\n",
      "Epoch 37: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5399 - accuracy: 0.8830 - val_loss: 0.9135 - val_accuracy: 0.7822 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 38/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5371 - accuracy: 0.8838\n",
      "Epoch 38: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5373 - accuracy: 0.8837 - val_loss: 0.8696 - val_accuracy: 0.7824 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 39/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5339 - accuracy: 0.8862\n",
      "Epoch 39: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5341 - accuracy: 0.8862 - val_loss: 0.6740 - val_accuracy: 0.8438 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 40/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8872\n",
      "Epoch 40: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5313 - accuracy: 0.8872 - val_loss: 0.6920 - val_accuracy: 0.8456 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 41/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8858\n",
      "Epoch 41: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5311 - accuracy: 0.8858 - val_loss: 0.7390 - val_accuracy: 0.8277 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 42/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5301 - accuracy: 0.8868\n",
      "Epoch 42: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5301 - accuracy: 0.8869 - val_loss: 0.9281 - val_accuracy: 0.7941 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 43/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.8892\n",
      "Epoch 43: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5261 - accuracy: 0.8892 - val_loss: 0.6891 - val_accuracy: 0.8412 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 44/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5310 - accuracy: 0.8889\n",
      "Epoch 44: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5310 - accuracy: 0.8888 - val_loss: 0.8756 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 45/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.8882\n",
      "Epoch 45: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5292 - accuracy: 0.8882 - val_loss: 0.8181 - val_accuracy: 0.8203 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 46/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.8897\n",
      "Epoch 46: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5228 - accuracy: 0.8897 - val_loss: 0.7314 - val_accuracy: 0.8298 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 47/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5234 - accuracy: 0.8892\n",
      "Epoch 47: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5235 - accuracy: 0.8892 - val_loss: 0.7809 - val_accuracy: 0.8241 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 48/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.8899\n",
      "Epoch 48: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5227 - accuracy: 0.8899 - val_loss: 0.7024 - val_accuracy: 0.8461 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 49/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5159 - accuracy: 0.8937\n",
      "Epoch 49: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5158 - accuracy: 0.8938 - val_loss: 0.6741 - val_accuracy: 0.8465 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 50/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.8903\n",
      "Epoch 50: val_accuracy did not improve from 0.85070\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5228 - accuracy: 0.8903 - val_loss: 0.7748 - val_accuracy: 0.8218 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 51/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5151 - accuracy: 0.8937\n",
      "Epoch 51: val_accuracy improved from 0.85070 to 0.85290, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.051.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5152 - accuracy: 0.8937 - val_loss: 0.6553 - val_accuracy: 0.8529 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 52/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.8912\n",
      "Epoch 52: val_accuracy did not improve from 0.85290\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5180 - accuracy: 0.8912 - val_loss: 0.6573 - val_accuracy: 0.8459 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 53/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5157 - accuracy: 0.8919\n",
      "Epoch 53: val_accuracy did not improve from 0.85290\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5157 - accuracy: 0.8919 - val_loss: 0.7010 - val_accuracy: 0.8408 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 54/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.8932\n",
      "Epoch 54: val_accuracy did not improve from 0.85290\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5144 - accuracy: 0.8931 - val_loss: 0.6591 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 55/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8957\n",
      "Epoch 55: val_accuracy improved from 0.85290 to 0.85600, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.055.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5108 - accuracy: 0.8957 - val_loss: 0.6379 - val_accuracy: 0.8560 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 56/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5141 - accuracy: 0.8926\n",
      "Epoch 56: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5143 - accuracy: 0.8925 - val_loss: 0.6894 - val_accuracy: 0.8454 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 57/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.8943\n",
      "Epoch 57: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5082 - accuracy: 0.8943 - val_loss: 0.6874 - val_accuracy: 0.8483 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 58/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5094 - accuracy: 0.8966\n",
      "Epoch 58: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5096 - accuracy: 0.8966 - val_loss: 0.7134 - val_accuracy: 0.8380 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 59/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5068 - accuracy: 0.8959\n",
      "Epoch 59: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5066 - accuracy: 0.8959 - val_loss: 0.6847 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 60/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5082 - accuracy: 0.8950\n",
      "Epoch 60: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5083 - accuracy: 0.8950 - val_loss: 0.7490 - val_accuracy: 0.8258 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 61/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.8981\n",
      "Epoch 61: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5028 - accuracy: 0.8981 - val_loss: 0.7885 - val_accuracy: 0.8262 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 62/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.8967\n",
      "Epoch 62: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5042 - accuracy: 0.8967 - val_loss: 0.6838 - val_accuracy: 0.8462 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 63/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5034 - accuracy: 0.8964\n",
      "Epoch 63: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5034 - accuracy: 0.8964 - val_loss: 0.6589 - val_accuracy: 0.8515 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 64/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5043 - accuracy: 0.8951\n",
      "Epoch 64: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.5044 - accuracy: 0.8951 - val_loss: 0.8209 - val_accuracy: 0.8082 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 65/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4948 - accuracy: 0.8995\n",
      "Epoch 65: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4950 - accuracy: 0.8994 - val_loss: 0.9633 - val_accuracy: 0.7920 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 66/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.5003 - accuracy: 0.8992\n",
      "Epoch 66: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5003 - accuracy: 0.8992 - val_loss: 0.7310 - val_accuracy: 0.8444 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 67/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.9006\n",
      "Epoch 67: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4989 - accuracy: 0.9006 - val_loss: 0.7255 - val_accuracy: 0.8294 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 68/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.8979\n",
      "Epoch 68: val_accuracy did not improve from 0.85600\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5015 - accuracy: 0.8979 - val_loss: 0.6801 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 69/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.8991\n",
      "Epoch 69: val_accuracy improved from 0.85600 to 0.87040, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.069.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4951 - accuracy: 0.8991 - val_loss: 0.6148 - val_accuracy: 0.8704 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 70/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4949 - accuracy: 0.9003\n",
      "Epoch 70: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4949 - accuracy: 0.9003 - val_loss: 0.8269 - val_accuracy: 0.8166 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 71/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.8995\n",
      "Epoch 71: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4951 - accuracy: 0.8995 - val_loss: 0.6551 - val_accuracy: 0.8522 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 72/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.9026\n",
      "Epoch 72: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4921 - accuracy: 0.9026 - val_loss: 0.6666 - val_accuracy: 0.8507 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 73/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.8993\n",
      "Epoch 73: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4937 - accuracy: 0.8994 - val_loss: 0.6753 - val_accuracy: 0.8517 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 74/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4960 - accuracy: 0.8993\n",
      "Epoch 74: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4962 - accuracy: 0.8992 - val_loss: 0.6086 - val_accuracy: 0.8639 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 75/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.9006\n",
      "Epoch 75: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.4928 - accuracy: 0.9006 - val_loss: 0.6684 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 76/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.4920 - accuracy: 0.9011\n",
      "Epoch 76: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4919 - accuracy: 0.9011 - val_loss: 0.7912 - val_accuracy: 0.8201 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 77/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4916 - accuracy: 0.9010\n",
      "Epoch 77: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4917 - accuracy: 0.9009 - val_loss: 0.6779 - val_accuracy: 0.8539 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 78/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.9017\n",
      "Epoch 78: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4890 - accuracy: 0.9017 - val_loss: 0.6344 - val_accuracy: 0.8596 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 79/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.4922 - accuracy: 0.9003\n",
      "Epoch 79: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4924 - accuracy: 0.9003 - val_loss: 0.7190 - val_accuracy: 0.8404 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 80/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4904 - accuracy: 0.9021\n",
      "Epoch 80: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4906 - accuracy: 0.9020 - val_loss: 0.6680 - val_accuracy: 0.8513 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 81/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.9033\n",
      "Epoch 81: val_accuracy did not improve from 0.87040\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4872 - accuracy: 0.9033 - val_loss: 0.7645 - val_accuracy: 0.8259 - lr: 0.0010\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.9319\n",
      "Epoch 82: val_accuracy improved from 0.87040 to 0.89890, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.082.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4034 - accuracy: 0.9319 - val_loss: 0.5055 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.9414\n",
      "Epoch 83: val_accuracy improved from 0.89890 to 0.90190, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.083.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3740 - accuracy: 0.9413 - val_loss: 0.4994 - val_accuracy: 0.9019 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.9463\n",
      "Epoch 84: val_accuracy improved from 0.90190 to 0.90650, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.084.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3573 - accuracy: 0.9463 - val_loss: 0.4879 - val_accuracy: 0.9065 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.9481\n",
      "Epoch 85: val_accuracy did not improve from 0.90650\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.3447 - accuracy: 0.9480 - val_loss: 0.4897 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.3329 - accuracy: 0.9504\n",
      "Epoch 86: val_accuracy did not improve from 0.90650\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.3329 - accuracy: 0.9505 - val_loss: 0.4841 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.9523\n",
      "Epoch 87: val_accuracy improved from 0.90650 to 0.90780, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.087.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.3257 - accuracy: 0.9523 - val_loss: 0.4778 - val_accuracy: 0.9078 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.9524\n",
      "Epoch 88: val_accuracy did not improve from 0.90780\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.3195 - accuracy: 0.9525 - val_loss: 0.4758 - val_accuracy: 0.9074 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.9557\n",
      "Epoch 89: val_accuracy improved from 0.90780 to 0.90830, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.089.h5.keras\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.3098 - accuracy: 0.9556 - val_loss: 0.4703 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.9556\n",
      "Epoch 90: val_accuracy did not improve from 0.90830\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.3067 - accuracy: 0.9556 - val_loss: 0.4733 - val_accuracy: 0.9051 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9567\n",
      "Epoch 91: val_accuracy did not improve from 0.90830\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2997 - accuracy: 0.9567 - val_loss: 0.4762 - val_accuracy: 0.9060 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.9574\n",
      "Epoch 92: val_accuracy improved from 0.90830 to 0.90890, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.092.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2948 - accuracy: 0.9574 - val_loss: 0.4722 - val_accuracy: 0.9089 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9579\n",
      "Epoch 93: val_accuracy did not improve from 0.90890\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2901 - accuracy: 0.9579 - val_loss: 0.4761 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9599\n",
      "Epoch 94: val_accuracy did not improve from 0.90890\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2839 - accuracy: 0.9599 - val_loss: 0.4716 - val_accuracy: 0.9073 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.9617\n",
      "Epoch 95: val_accuracy did not improve from 0.90890\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2795 - accuracy: 0.9617 - val_loss: 0.4622 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2750 - accuracy: 0.9610\n",
      "Epoch 96: val_accuracy did not improve from 0.90890\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2751 - accuracy: 0.9610 - val_loss: 0.4666 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9625\n",
      "Epoch 97: val_accuracy improved from 0.90890 to 0.91240, saving model to /home/studio-lab-user/sagemaker-studiolab-notebooks/saved_models/cifar10_ResNet_model.097.h5.keras\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2700 - accuracy: 0.9625 - val_loss: 0.4568 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2660 - accuracy: 0.9636\n",
      "Epoch 98: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2660 - accuracy: 0.9635 - val_loss: 0.4633 - val_accuracy: 0.9078 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9646\n",
      "Epoch 99: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2596 - accuracy: 0.9646 - val_loss: 0.4603 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9644\n",
      "Epoch 100: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2570 - accuracy: 0.9644 - val_loss: 0.4602 - val_accuracy: 0.9100 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9652\n",
      "Epoch 101: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2558 - accuracy: 0.9652 - val_loss: 0.4546 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9658\n",
      "Epoch 102: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2511 - accuracy: 0.9658 - val_loss: 0.4569 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9670\n",
      "Epoch 103: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2462 - accuracy: 0.9670 - val_loss: 0.4626 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9669\n",
      "Epoch 104: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2434 - accuracy: 0.9669 - val_loss: 0.4618 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9674\n",
      "Epoch 105: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2407 - accuracy: 0.9674 - val_loss: 0.4631 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9680\n",
      "Epoch 106: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2377 - accuracy: 0.9680 - val_loss: 0.4576 - val_accuracy: 0.9103 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9674\n",
      "Epoch 107: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2370 - accuracy: 0.9674 - val_loss: 0.4672 - val_accuracy: 0.9071 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9679\n",
      "Epoch 108: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2335 - accuracy: 0.9679 - val_loss: 0.4625 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9692\n",
      "Epoch 109: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2307 - accuracy: 0.9692 - val_loss: 0.4670 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9700\n",
      "Epoch 110: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2269 - accuracy: 0.9700 - val_loss: 0.4850 - val_accuracy: 0.9035 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9698\n",
      "Epoch 111: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2278 - accuracy: 0.9698 - val_loss: 0.4634 - val_accuracy: 0.9069 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9711\n",
      "Epoch 112: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2226 - accuracy: 0.9711 - val_loss: 0.4538 - val_accuracy: 0.9104 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/120\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2200 - accuracy: 0.9715\n",
      "Epoch 113: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2200 - accuracy: 0.9715 - val_loss: 0.4501 - val_accuracy: 0.9100 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9717\n",
      "Epoch 114: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2182 - accuracy: 0.9717 - val_loss: 0.4591 - val_accuracy: 0.9096 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9718\n",
      "Epoch 115: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2151 - accuracy: 0.9718 - val_loss: 0.4585 - val_accuracy: 0.9081 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9717\n",
      "Epoch 116: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2151 - accuracy: 0.9717 - val_loss: 0.4690 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/120\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9729\n",
      "Epoch 117: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2109 - accuracy: 0.9729 - val_loss: 0.4646 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/120\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9727\n",
      "Epoch 118: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.2083 - accuracy: 0.9727 - val_loss: 0.4649 - val_accuracy: 0.9084 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.9730\n",
      "Epoch 119: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2068 - accuracy: 0.9730 - val_loss: 0.4640 - val_accuracy: 0.9073 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/120\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9737\n",
      "Epoch 120: val_accuracy did not improve from 0.91240\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.2063 - accuracy: 0.9737 - val_loss: 0.4809 - val_accuracy: 0.9060 - lr: 1.0000e-04\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4809 - accuracy: 0.9060\n",
      "Test loss: 0.48086869716644287\n",
      "Test accuracy: 0.906000018119812\n"
     ]
    }
   ],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              validation_data=(x_test, y_test),\n",
    "              epochs=epochs, verbose=1, workers=4,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y2dfFyJPR9hF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4809 - accuracy: 0.9060\n",
      "Test loss: 0.48086869716644287\n",
      "Test accuracy: 0.906000018119812\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
